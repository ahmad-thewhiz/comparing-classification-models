{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from time import time\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the Neural Network architecture\n",
    "class SentimentANN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256], dropout=0.5):\n",
    "        super(SentimentANN, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))  # Output layer\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def main(dataset_dir, output_dir, batch_size=1024, epochs=10, learning_rate=1e-3, feature_dim=500):\n",
    "    # Check device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Importing the datasets\n",
    "    try:\n",
    "        print(\"Loading training data...\")\n",
    "        df_train = pd.read_csv(os.path.join(dataset_dir, 'train_data.csv'))\n",
    "        print(\"Loading testing data...\")\n",
    "        df_test = pd.read_csv(os.path.join(dataset_dir, 'test_data.csv'))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Error: {e}')\n",
    "        sys.exit(1)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(f'Error: One of the CSV files is empty or malformed: {e}')\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f'An unexpected error occurred while reading the datasets: {e}')\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nDataset directory: {dataset_dir}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    print(f\"\\nTraining Dataset has {df_train.shape[0]} rows and {df_train.shape[1]} columns\")\n",
    "    print(f\"Testing Dataset has {df_test.shape[0]} rows and {df_test.shape[1]} columns\")\n",
    "\n",
    "    # Display first 5 rows of training data\n",
    "    print(\"\\nFirst 5 rows of Training Data:\")\n",
    "    print(df_train.head(5))\n",
    "\n",
    "    # Check if required columns exist\n",
    "    required_columns = ['sentence', 'sentiment']\n",
    "    for col in required_columns:\n",
    "        if col not in df_train.columns or col not in df_test.columns:\n",
    "            print(f\"Error: '{col}' column not found in one of the datasets.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    # Sentiment distribution\n",
    "    sentiment_counts_train = df_train['sentiment'].value_counts()\n",
    "    print(\"\\nSentiment distribution in Training Data:\")\n",
    "    print(sentiment_counts_train)\n",
    "\n",
    "    sentiment_counts_test = df_test['sentiment'].value_counts()\n",
    "    print(\"\\nSentiment distribution in Testing Data:\")\n",
    "    print(sentiment_counts_test)\n",
    "\n",
    "    # Prepare data\n",
    "    train_sentences = df_train['sentence'].astype(str).values\n",
    "    train_labels = df_train['sentiment'].values\n",
    "    test_sentences = df_test['sentence'].astype(str).values\n",
    "    test_labels = df_test['sentiment'].values\n",
    "\n",
    "    # Initialize TfidfVectorizer with dimensionality reduction\n",
    "    tf_vec = TfidfVectorizer(\n",
    "        max_features=10000,  # Limit to top 10k features to manage memory\n",
    "        ngram_range=(1, 2),   # Use unigrams and bigrams\n",
    "        stop_words='english',\n",
    "        lowercase=True,\n",
    "        strip_accents='unicode',\n",
    "        min_df=50,\n",
    "        max_df=0.7\n",
    "    )\n",
    "\n",
    "    # Transform training data\n",
    "    print(\"\\nStarting TF-IDF vectorization on training data...\")\n",
    "    start = time()\n",
    "    X_train_tf = tf_vec.fit_transform(train_sentences)\n",
    "    end = time()\n",
    "    print(f\"Time to transform training data: {end - start:.2f}s\")\n",
    "    print(f\"Training TF-IDF shape: {X_train_tf.shape}\")\n",
    "\n",
    "    # Transform testing data\n",
    "    print(\"\\nStarting TF-IDF vectorization on testing data...\")\n",
    "    start = time()\n",
    "    X_test_tf = tf_vec.transform(test_sentences)\n",
    "    duration = time() - start\n",
    "    print(f\"Time taken to transform testing data: {duration:.2f} seconds\")\n",
    "    print(f\"Testing TF-IDF shape: {X_test_tf.shape}\")\n",
    "\n",
    "    # Dimensionality Reduction with TruncatedSVD\n",
    "    print(\"\\nStarting dimensionality reduction with TruncatedSVD...\")\n",
    "    svd = TruncatedSVD(n_components=feature_dim, random_state=42)\n",
    "    start = time()\n",
    "    X_train_svd = svd.fit_transform(X_train_tf)\n",
    "    X_test_svd = svd.transform(X_test_tf)\n",
    "    end = time()\n",
    "    print(f\"Time for TruncatedSVD: {end - start:.2f}s\")\n",
    "    print(f\"Reduced feature shape: {X_train_svd.shape}\")\n",
    "\n",
    "    # Save the vectorizer and SVD transformer\n",
    "    with open(os.path.join(output_dir, 'tfidf_vectorizer.pkl'), 'wb') as vec_file:\n",
    "        pickle.dump(tf_vec, vec_file)\n",
    "    with open(os.path.join(output_dir, 'svd_transformer.pkl'), 'wb') as svd_file:\n",
    "        pickle.dump(svd, svd_file)\n",
    "    print(\"TfidfVectorizer and TruncatedSVD saved.\")\n",
    "\n",
    "    # Convert data to tensors\n",
    "    print(\"\\nConverting data to PyTorch tensors...\")\n",
    "    X_train_tensor = torch.tensor(X_train_svd, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
    "    X_test_tensor = torch.tensor(X_test_svd, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(test_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # Create TensorDatasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    print(\"DataLoaders created.\")\n",
    "\n",
    "    # Initialize the model\n",
    "    input_dim = feature_dim\n",
    "    hidden_dims = [512, 256]\n",
    "    dropout = 0.5\n",
    "    model = SentimentANN(input_dim=input_dim, hidden_dims=hidden_dims, dropout=dropout)\n",
    "    model.to(device)\n",
    "    print(\"\\nModel architecture:\")\n",
    "    print(model)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    print(\"\\nStarting training...\")\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    all_probs = np.array(all_probs).flatten()\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    print(f\"\\nAccuracy on test set: {accuracy * 100:.2f}%\")\n",
    "    print(f\"AUC-ROC on test set: {auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title(\"Confusion Matrix Heatmap\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    cm_path = os.path.join(output_dir, 'confusion_matrix_heatmap.png')\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix heatmap saved as '{cm_path}'\")\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    roc_path = os.path.join(output_dir, 'roc_curve.png')\n",
    "    plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"ROC curve saved as '{roc_path}'\")\n",
    "\n",
    "    # Save the model\n",
    "    model_path = os.path.join(output_dir, 'sentiment_ann_model.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved as '{model_path}'\")\n",
    "\n",
    "    print(\"\\nTraining and evaluation completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading training data...\n",
      "Loading testing data...\n",
      "\n",
      "Dataset directory: /home/dgxuser16/NTL/mccarthy/ahmad/Projects/ML_Course_Proj/data/twitter\n",
      "Output directory: output\n",
      "\n",
      "Training Dataset has 15000 rows and 2 columns\n",
      "Testing Dataset has 359 rows and 2 columns\n",
      "\n",
      "First 5 rows of Training Data:\n",
      "                                                  sentence  sentiment\n",
      "1110964  chillin at the rooftoop on a rainy sunday than...          1\n",
      "442422   me loves you too fran i don t feel too good ri...          0\n",
      "348915   u guys r so funny p boston tonight so close ma...          0\n",
      "575434   can t sleep i had that chance with that chunky...          0\n",
      "289960   i miss my tv it s at my apartment in san anton...          0\n",
      "\n",
      "Sentiment distribution in Training Data:\n",
      "sentiment\n",
      "0    7564\n",
      "1    7436\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sentiment distribution in Testing Data:\n",
      "sentiment\n",
      "1    182\n",
      "0    177\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Starting TF-IDF vectorization on training data...\n",
      "Time to transform training data: 0.30s\n",
      "Training TF-IDF shape: (15000, 10000)\n",
      "\n",
      "Starting TF-IDF vectorization on testing data...\n",
      "Time taken to transform testing data: 0.00 seconds\n",
      "Testing TF-IDF shape: (359, 10000)\n",
      "\n",
      "Starting dimensionality reduction with TruncatedSVD...\n",
      "Time for TruncatedSVD: 5.93s\n",
      "Reduced feature shape: (15000, 600)\n",
      "TfidfVectorizer and TruncatedSVD saved.\n",
      "\n",
      "Converting data to PyTorch tensors...\n",
      "DataLoaders created.\n",
      "\n",
      "Model architecture:\n",
      "SentimentANN(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=600, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 938/938 [00:03<00:00, 276.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.5945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 938/938 [00:03<00:00, 292.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Loss: 0.5338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 938/938 [00:03<00:00, 293.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Loss: 0.5086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 938/938 [00:03<00:00, 292.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Loss: 0.4736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 938/938 [00:03<00:00, 293.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Loss: 0.4238\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 23/23 [00:00<00:00, 67.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on test set: 74.65%\n",
      "AUC-ROC on test set: 0.8336\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7443    0.7401    0.7422       177\n",
      "         1.0     0.7486    0.7527    0.7507       182\n",
      "\n",
      "    accuracy                         0.7465       359\n",
      "   macro avg     0.7465    0.7464    0.7464       359\n",
      "weighted avg     0.7465    0.7465    0.7465       359\n",
      "\n",
      "Confusion matrix heatmap saved as 'output/confusion_matrix_heatmap.png'\n",
      "ROC curve saved as 'output/roc_curve.png'\n",
      "Model saved as 'output/sentiment_ann_model.pth'\n",
      "\n",
      "Training and evaluation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "main(\n",
    "        dataset_dir='/home/dgxuser16/NTL/mccarthy/ahmad/Projects/ML_Course_Proj/data/twitter', \n",
    "        output_dir='output', \n",
    "        batch_size=8, \n",
    "        epochs=25, \n",
    "        learning_rate=0.001,\n",
    "        feature_dim=600\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
