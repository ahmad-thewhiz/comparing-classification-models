{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC  # Changed from BernoulliNB to SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from time import time\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "def main(dataset_dir, output_dir):\n",
    "\n",
    "    # Importing the datasets\n",
    "    try:\n",
    "        df_train = pd.read_csv(f'{dataset_dir}/train_data.csv').sample(n=15000, random_state=42)\n",
    "        df_test = pd.read_csv(f'{dataset_dir}/test_data.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Train and Test datasets not found in the specified directory: {e}')\n",
    "        return  # Exit the function if datasets are not found\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Dataset directory: {dataset_dir}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    print(\"Training Dataset has {} rows and {} columns\".format(df_train.shape[0], df_train.shape[1]))\n",
    "    print(\"Testing Dataset has {} rows and {} columns\".format(df_test.shape[0], df_test.shape[1]))\n",
    "\n",
    "    print(\"\\nFirst 5 rows of Training Dataset:\")\n",
    "    print(df_train.head(5))\n",
    "    sentiment_counts_train = df_train['sentiment'].value_counts()\n",
    "    print(\"\\nSentiment distribution in Training Dataset:\")\n",
    "    print(sentiment_counts_train)\n",
    "\n",
    "    print(\"\\nFirst 5 rows of Testing Dataset:\")\n",
    "    print(df_test.head(5))\n",
    "    sentiment_counts_test = df_test['sentiment'].value_counts()\n",
    "    print(\"\\nSentiment distribution in Testing Dataset:\")\n",
    "    print(sentiment_counts_test)\n",
    "\n",
    "    trainX = df_train['sentence']\n",
    "    trainY = df_train['sentiment']\n",
    "    testX = df_test['sentence']\n",
    "    testY = df_test['sentiment']\n",
    "\n",
    "    tf_vec = TfidfVectorizer()\n",
    "    start = time()\n",
    "    X_train_tf = tf_vec.fit_transform(trainX)\n",
    "    end = time()\n",
    "    print('\\nTime to transform training data: {:.2f}s'.format(end - start))\n",
    "    print(\"n_samples: {}, n_features: {}\".format(X_train_tf.shape[0], X_train_tf.shape[1]))\n",
    "\n",
    "    start = time()\n",
    "    X_test_tf = tf_vec.transform(testX)\n",
    "    duration = time() - start\n",
    "    print(\"Time taken to extract features from test data: {:.2f} seconds\".format(duration))\n",
    "    print(\"n_samples: {}, n_features: {}\".format(X_test_tf.shape[0], X_test_tf.shape[1]))\n",
    "\n",
    "\n",
    "    # Defining the parameter grid for SVM\n",
    "\n",
    "    # param_grid = {\n",
    "    #     'C': [0.1, 1, 10],  # Regularization parameter\n",
    "    #     'kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],  # Kernel type\n",
    "    #     'gamma': ['scale', 'auto']  # Kernel coefficient\n",
    "    # }\n",
    "\n",
    "    param_grid = [\n",
    "        {\n",
    "            'kernel': ['linear'],\n",
    "            'C': [0.1, 1, 10]  # Regularization parameter\n",
    "        },\n",
    "        {\n",
    "            'kernel': ['rbf', 'poly'],\n",
    "            'C': [0.1, 1, 10],  # Regularization parameter\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]  # Kernel coefficient\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Initialize the SVM classifier\n",
    "    svm_classifier = SVC()\n",
    "\n",
    "    # Setting up GridSearchCV to find the best parameters\n",
    "    grid_search = GridSearchCV(estimator=svm_classifier, \n",
    "                               param_grid=param_grid, \n",
    "                               scoring='accuracy', \n",
    "                               cv=3, \n",
    "                               verbose=2, \n",
    "                               n_jobs=-1) \n",
    "\n",
    "    # Start the grid search\n",
    "    print(\"\\nStarting GridSearchCV for SVM...\")\n",
    "    start = time()\n",
    "    grid_search.fit(X_train_tf, trainY)\n",
    "    end = time()\n",
    "    print(f\"GridSearchCV completed in {end - start:.2f}s\")\n",
    "\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    print(\"\\nBest cross-validation accuracy:\")\n",
    "    print(f\"{grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Evaluate the model with the best parameters on the test set\n",
    "    best_svm_classifier = grid_search.best_estimator_\n",
    "    start = time()\n",
    "    y1_predict = best_svm_classifier.predict(X_test_tf)\n",
    "    prediction_time = time() - start\n",
    "    print(\"Prediction time: {:.4f}s\".format(prediction_time))\n",
    "\n",
    "    acc = metrics.accuracy_score(testY, y1_predict)\n",
    "    print(f\"Accuracy on test set: {acc*100:.2f}%\")\n",
    "\n",
    "    print(\"\\nClassification report for the optimized classifier: \\n\")\n",
    "    print(metrics.classification_report(testY, y1_predict))\n",
    "\n",
    "    # Create a heatmap\n",
    "    conf_matrix = confusion_matrix(testY, y1_predict)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "\n",
    "    plt.title(\"Confusion Matrix Heatmap\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "\n",
    "    plt.savefig(f'{output_dir}/confusion_matrix_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the figure to free memory\n",
    "\n",
    "    # Save the model and the vectorizer\n",
    "    with open(f'{output_dir}/best_svm_classifier_model.pkl', 'wb') as model_file:\n",
    "        pickle.dump(best_svm_classifier, model_file)\n",
    "    \n",
    "    with open(f'{output_dir}/tfidf_vectorizer.pkl', 'wb') as vec_file:\n",
    "        pickle.dump(tf_vec, vec_file)\n",
    "\n",
    "    print(\"Model saved as 'best_svm_classifier_model.pkl'\")\n",
    "    print(\"TF-IDF Vectorizer saved as 'tfidf_vectorizer.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC  # Changed from BernoulliNB to SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Removed GridSearchCV as it's no longer needed\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from time import time\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "def main(dataset_dir, output_dir):\n",
    "\n",
    "    # Importing the datasets\n",
    "    try:\n",
    "        df_train = pd.read_csv(f'{dataset_dir}/train_data.csv')\n",
    "        df_test = pd.read_csv(f'{dataset_dir}/test_data.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Train and Test datasets not found in the specified directory: {e}')\n",
    "        return  # Exit the function if datasets are not found\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Dataset directory: {dataset_dir}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    print(\"Training Dataset has {} rows and {} columns\".format(df_train.shape[0], df_train.shape[1]))\n",
    "    print(\"Testing Dataset has {} rows and {} columns\".format(df_test.shape[0], df_test.shape[1]))\n",
    "\n",
    "    print(\"\\nFirst 5 rows of Training Dataset:\")\n",
    "    print(df_train.head(5))\n",
    "    sentiment_counts_train = df_train['sentiment'].value_counts()\n",
    "    print(\"\\nSentiment distribution in Training Dataset:\")\n",
    "    print(sentiment_counts_train)\n",
    "\n",
    "    print(\"\\nFirst 5 rows of Testing Dataset:\")\n",
    "    print(df_test.head(5))\n",
    "    sentiment_counts_test = df_test['sentiment'].value_counts()\n",
    "    print(\"\\nSentiment distribution in Testing Dataset:\")\n",
    "    print(sentiment_counts_test)\n",
    "\n",
    "    trainX = df_train['sentence']\n",
    "    trainY = df_train['sentiment']\n",
    "    testX = df_test['sentence']\n",
    "    testY = df_test['sentiment']\n",
    "\n",
    "    tf_vec = TfidfVectorizer()\n",
    "    start = time()\n",
    "    X_train_tf = tf_vec.fit_transform(trainX)\n",
    "    end = time()\n",
    "    print('\\nTime to transform training data: {:.2f}s'.format(end - start))\n",
    "    print(\"n_samples: {}, n_features: {}\".format(X_train_tf.shape[0], X_train_tf.shape[1]))\n",
    "\n",
    "    start = time()\n",
    "    X_test_tf = tf_vec.transform(testX)\n",
    "    duration = time() - start\n",
    "    print(\"Time taken to extract features from test data: {:.2f} seconds\".format(duration))\n",
    "    print(\"n_samples: {}, n_features: {}\".format(X_test_tf.shape[0], X_test_tf.shape[1]))\n",
    "\n",
    "    # Initialize the SVM classifier with predefined hyperparameters\n",
    "    # You can adjust 'C', 'kernel', and 'gamma' as needed\n",
    "    svm_classifier = SVC(\n",
    "        C=1.0,            # Regularization parameter\n",
    "        kernel='rbf',  # Kernel type: 'linear', 'poly', 'rbf', 'sigmoid', etc.\n",
    "        gamma=1.0,    # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "        random_state=42    # Seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # Train the SVM classifier\n",
    "    print(\"\\nTraining the SVM classifier...\")\n",
    "    start = time()\n",
    "    svm_classifier.fit(X_train_tf, trainY)\n",
    "    end = time()\n",
    "    print(f\"SVM training completed in {end - start:.2f}s\")\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    start = time()\n",
    "    y_pred = svm_classifier.predict(X_test_tf)\n",
    "    prediction_time = time() - start\n",
    "    print(\"Prediction time: {:.4f}s\".format(prediction_time))\n",
    "\n",
    "    acc = metrics.accuracy_score(testY, y_pred)\n",
    "    print(f\"Accuracy on test set: {acc*100:.2f}%\")\n",
    "\n",
    "    print(\"\\nClassification report for the SVM classifier: \\n\")\n",
    "    print(metrics.classification_report(testY, y_pred))\n",
    "\n",
    "    # Create a heatmap for the confusion matrix\n",
    "    conf_matrix = confusion_matrix(testY, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "\n",
    "    plt.title(\"Confusion Matrix Heatmap\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "\n",
    "    plt.savefig(f'{output_dir}/confusion_matrix_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the figure to free memory\n",
    "\n",
    "    # Save the model and the vectorizer\n",
    "    with open(f'{output_dir}/svm_classifier_model.pkl', 'wb') as model_file:\n",
    "        pickle.dump(svm_classifier, model_file)\n",
    "    \n",
    "    with open(f'{output_dir}/tfidf_vectorizer.pkl', 'wb') as vec_file:\n",
    "        pickle.dump(tf_vec, vec_file)\n",
    "\n",
    "    print(\"Model saved as 'svm_classifier_model.pkl'\")\n",
    "    print(\"TF-IDF Vectorizer saved as 'tfidf_vectorizer.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /home/dgxuser16/NTL/mccarthy/ahmad/Projects/ML_Course_Proj/data/twitter\n",
      "Output directory: output\n",
      "Training Dataset has 1523665 rows and 2 columns\n",
      "Testing Dataset has 359 rows and 2 columns\n",
      "\n",
      "First 5 rows of Training Dataset:\n",
      "                                            sentence  sentiment\n",
      "0  awww that s a bummer you shoulda got david car...          0\n",
      "1  is upset that he can t update his facebook by ...          0\n",
      "2  i dived many times for the ball managed to sav...          0\n",
      "3     my whole body feels itchy and like its on fire          0\n",
      "4  no it s not behaving at all i m mad why am i h...          0\n",
      "\n",
      "Sentiment distribution in Training Dataset:\n",
      "sentiment\n",
      "0    766980\n",
      "1    756685\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 5 rows of Testing Dataset:\n",
      "                                            sentence  sentiment\n",
      "0  i loooooooovvvvvveee my kindle not that the dx...          1\n",
      "1  reading my kindle love it lee childs is good read          1\n",
      "2  ok first assesment of the kindle it fucking rocks          1\n",
      "3  you ll love your kindle i ve had mine for a fe...          1\n",
      "4  fair enough but i have the kindle and i think ...          1\n",
      "\n",
      "Sentiment distribution in Testing Dataset:\n",
      "sentiment\n",
      "1    182\n",
      "0    177\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time to transform training data: 16.10s\n",
      "n_samples: 1523665, n_features: 266876\n",
      "Time taken to extract features from test data: 0.01 seconds\n",
      "n_samples: 359, n_features: 266876\n",
      "\n",
      "Training the SVM classifier...\n"
     ]
    }
   ],
   "source": [
    "main('/home/dgxuser16/NTL/mccarthy/ahmad/Projects/ML_Course_Proj/data/twitter', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /home/dgxuser16/NTL/mccarthy/ahmad/Projects/ML_Course_Proj/data/twitter\n",
      "Output directory: output\n",
      "Training Dataset has 50000 rows and 2 columns\n",
      "Testing Dataset has 359 rows and 2 columns\n",
      "\n",
      "First 5 rows of Training Dataset:\n",
      "                                                  sentence  sentiment\n",
      "1110964  chillin at the rooftoop on a rainy sunday than...          1\n",
      "442422   me loves you too fran i don t feel too good ri...          0\n",
      "348915   u guys r so funny p boston tonight so close ma...          0\n",
      "575434   can t sleep i had that chance with that chunky...          0\n",
      "289960   i miss my tv it s at my apartment in san anton...          0\n",
      "\n",
      "Sentiment distribution in Training Dataset:\n",
      "sentiment\n",
      "0    25250\n",
      "1    24750\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 5 rows of Testing Dataset:\n",
      "                                            sentence  sentiment\n",
      "0  i loooooooovvvvvveee my kindle not that the dx...          1\n",
      "1  reading my kindle love it lee childs is good read          1\n",
      "2  ok first assesment of the kindle it fucking rocks          1\n",
      "3  you ll love your kindle i ve had mine for a fe...          1\n",
      "4  fair enough but i have the kindle and i think ...          1\n",
      "\n",
      "Sentiment distribution in Testing Dataset:\n",
      "sentiment\n",
      "1    182\n",
      "0    177\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time to transform training data: 0.43s\n",
      "n_samples: 50000, n_features: 3940\n",
      "Time taken to extract features from test data: 0.00 seconds\n",
      "n_samples: 359, n_features: 3940\n",
      "\n",
      "Training the SVM classifier...\n",
      "SVM training completed in 409.34s\n",
      "Prediction time: 1.0254s\n",
      "Accuracy on test set: 78.55%\n",
      "\n",
      "Classification report for the SVM classifier: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.75      0.77       177\n",
      "           1       0.77      0.82      0.80       182\n",
      "\n",
      "    accuracy                           0.79       359\n",
      "   macro avg       0.79      0.78      0.78       359\n",
      "weighted avg       0.79      0.79      0.79       359\n",
      "\n",
      "Model saved as 'svm_classifier_model.pkl'\n",
      "TF-IDF Vectorizer saved as 'tfidf_vectorizer.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC  # Changed from BernoulliNB to SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Removed GridSearchCV as it's no longer needed\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from time import time\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "def main(dataset_dir, output_dir):\n",
    "\n",
    "    # Importing the datasets\n",
    "    try:\n",
    "        df_train = pd.read_csv(f'{dataset_dir}/train_data.csv').sample(n=50000, random_state=42)\n",
    "        df_test = pd.read_csv(f'{dataset_dir}/test_data.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Train and Test datasets not found in the specified directory: {e}')\n",
    "        return  # Exit the function if datasets are not found\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Dataset directory: {dataset_dir}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    print(\"Training Dataset has {} rows and {} columns\".format(df_train.shape[0], df_train.shape[1]))\n",
    "    print(\"Testing Dataset has {} rows and {} columns\".format(df_test.shape[0], df_test.shape[1]))\n",
    "\n",
    "    print(\"\\nFirst 5 rows of Training Dataset:\")\n",
    "    print(df_train.head(5))\n",
    "    sentiment_counts_train = df_train['sentiment'].value_counts()\n",
    "    print(\"\\nSentiment distribution in Training Dataset:\")\n",
    "    print(sentiment_counts_train)\n",
    "\n",
    "    print(\"\\nFirst 5 rows of Testing Dataset:\")\n",
    "    print(df_test.head(5))\n",
    "    sentiment_counts_test = df_test['sentiment'].value_counts()\n",
    "    print(\"\\nSentiment distribution in Testing Dataset:\")\n",
    "    print(sentiment_counts_test)\n",
    "\n",
    "    trainX = df_train['sentence']\n",
    "    trainY = df_train['sentiment']\n",
    "    testX = df_test['sentence']\n",
    "    testY = df_test['sentiment']\n",
    "\n",
    "    # tf_vec = TfidfVectorizer()\n",
    "    tf_vec = TfidfVectorizer(\n",
    "    min_df=10,           # Ignore terms that appear in fewer than 5 documents\n",
    "    max_df=0.7          # Ignore terms that appear in more than 70% of documents\n",
    "    )\n",
    "\n",
    "    start = time()\n",
    "    X_train_tf = tf_vec.fit_transform(trainX)\n",
    "    end = time()\n",
    "    print('\\nTime to transform training data: {:.2f}s'.format(end - start))\n",
    "    print(\"n_samples: {}, n_features: {}\".format(X_train_tf.shape[0], X_train_tf.shape[1]))\n",
    "\n",
    "    start = time()\n",
    "    X_test_tf = tf_vec.transform(testX)\n",
    "    duration = time() - start\n",
    "    print(\"Time taken to extract features from test data: {:.2f} seconds\".format(duration))\n",
    "    print(\"n_samples: {}, n_features: {}\".format(X_test_tf.shape[0], X_test_tf.shape[1]))\n",
    "\n",
    "    # Initialize the SVM classifier with predefined hyperparameters\n",
    "    # You can adjust 'C', 'kernel', and 'gamma' as needed\n",
    "    svm_classifier = SVC(\n",
    "        C=1.0,            # Regularization parameter\n",
    "        kernel='rbf',  # Kernel type: 'linear', 'poly', 'rbf', 'sigmoid', etc.\n",
    "        gamma=1.0,    # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "        random_state=42    # Seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # Train the SVM classifier\n",
    "    print(\"\\nTraining the SVM classifier...\")\n",
    "    start = time()\n",
    "    svm_classifier.fit(X_train_tf, trainY)\n",
    "    end = time()\n",
    "    print(f\"SVM training completed in {end - start:.2f}s\")\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    start = time()\n",
    "    y_pred = svm_classifier.predict(X_test_tf)\n",
    "    prediction_time = time() - start\n",
    "    print(\"Prediction time: {:.4f}s\".format(prediction_time))\n",
    "\n",
    "    acc = metrics.accuracy_score(testY, y_pred)\n",
    "    print(f\"Accuracy on test set: {acc*100:.2f}%\")\n",
    "\n",
    "    print(\"\\nClassification report for the SVM classifier: \\n\")\n",
    "    print(metrics.classification_report(testY, y_pred))\n",
    "\n",
    "    # Create a heatmap for the confusion matrix\n",
    "    conf_matrix = confusion_matrix(testY, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "\n",
    "    plt.title(\"Confusion Matrix Heatmap\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "\n",
    "    plt.savefig(f'{output_dir}/confusion_matrix_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the figure to free memory\n",
    "\n",
    "    # Save the model and the vectorizer\n",
    "    with open(f'{output_dir}/svm_classifier_model.pkl', 'wb') as model_file:\n",
    "        pickle.dump(svm_classifier, model_file)\n",
    "    \n",
    "    with open(f'{output_dir}/tfidf_vectorizer.pkl', 'wb') as vec_file:\n",
    "        pickle.dump(tf_vec, vec_file)\n",
    "\n",
    "    print(\"Model saved as 'svm_classifier_model.pkl'\")\n",
    "    print(\"TF-IDF Vectorizer saved as 'tfidf_vectorizer.pkl'\")\n",
    "\n",
    "main('/home/dgxuser16/NTL/mccarthy/ahmad/Projects/ML_Course_Proj/data/twitter', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
