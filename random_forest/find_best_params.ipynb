{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from time import time\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def main(dataset_dir, output_dir):\n",
    "    # Importing the datasets\n",
    "    try:\n",
    "        df_train = pd.read_csv(os.path.join(dataset_dir, 'train_data.csv')).sample(n=25000, random_state=42)\n",
    "        df_test = pd.read_csv(os.path.join(dataset_dir, 'test_data.csv'))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Error: {e}')\n",
    "        sys.exit(1)  # Exit the script if datasets are not found\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(f'Error: One of the CSV files is empty or malformed: {e}')\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f'An unexpected error occurred while reading the datasets: {e}')\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Dataset directory: {dataset_dir}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    print(f\"\\nTraining Dataset has {df_train.shape[0]} rows and {df_train.shape[1]} columns\")\n",
    "    print(f\"Testing Dataset has {df_test.shape[0]} rows and {df_test.shape[1]} columns\")\n",
    "\n",
    "    # Display first 5 rows of training data\n",
    "    print(\"\\nFirst 5 rows of Training Data:\")\n",
    "    print(df_train.head(5))\n",
    "\n",
    "    # Check if 'sentiment' column exists\n",
    "    if 'sentiment' not in df_train.columns or 'sentiment' not in df_test.columns:\n",
    "        print(\"Error: 'sentiment' column not found in one of the datasets.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    sentiment_counts_train = df_train['sentiment'].value_counts()\n",
    "    print(\"\\nSentiment distribution in Training Data:\")\n",
    "    print(sentiment_counts_train)\n",
    "\n",
    "    # Display first 5 rows of testing data\n",
    "    print(\"\\nFirst 5 rows of Testing Data:\")\n",
    "    print(df_test.head(5))\n",
    "\n",
    "    sentiment_counts_test = df_test['sentiment'].value_counts()\n",
    "    print(\"\\nSentiment distribution in Testing Data:\")\n",
    "    print(sentiment_counts_test)\n",
    "\n",
    "    # Check if 'sentence' column exists\n",
    "    if 'sentence' not in df_train.columns or 'sentence' not in df_test.columns:\n",
    "        print(\"Error: 'sentence' column not found in one of the datasets.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    trainX = df_train['sentence'].astype(str)  # Ensure all entries are strings\n",
    "    trainY = df_train['sentiment']\n",
    "    testX = df_test['sentence'].astype(str)\n",
    "    testY = df_test['sentiment']\n",
    "\n",
    "    # Initialize TfidfVectorizer with appropriate parameters to handle large datasets\n",
    "    tf_vec = TfidfVectorizer()\n",
    "    # tf_vec = TfidfVectorizer(\n",
    "    #     max_features=100000,  # Limit to top 100k features to manage memory\n",
    "    #     ngram_range=(1, 2),    # Use unigrams and bigrams\n",
    "    #     stop_words='english',  # Remove English stop words\n",
    "    #     lowercase=True,\n",
    "    #     strip_accents='unicode'\n",
    "    # )\n",
    "\n",
    "    # Transform training data\n",
    "    print(\"\\nStarting TF-IDF vectorization on training data...\")\n",
    "    start = time()\n",
    "    X_train_tf = tf_vec.fit_transform(trainX)\n",
    "    end = time()\n",
    "    print(f\"Time to transform training data: {end - start:.2f}s\")\n",
    "    print(f\"Training Data Shape: n_samples={X_train_tf.shape[0]}, n_features={X_train_tf.shape[1]}\")\n",
    "\n",
    "    # Transform testing data\n",
    "    print(\"\\nStarting TF-IDF vectorization on testing data...\")\n",
    "    start = time()\n",
    "    X_test_tf = tf_vec.transform(testX)\n",
    "    duration = time() - start\n",
    "    print(f\"Time taken to extract features from test data: {duration:.2f} seconds\")\n",
    "    print(f\"Testing Data Shape: n_samples={X_test_tf.shape[0]}, n_features={X_test_tf.shape[1]}\")\n",
    "\n",
    "    # Defining the parameter grid for Random Forest\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Initialize the Random Forest Classifier\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Setting up GridSearchCV to find the best parameters\n",
    "    random_search = GridSearchCV(\n",
    "        estimator=rf_classifier,\n",
    "        param_grid=param_dist,\n",
    "        scoring='roc_auc',  # Optimize for AUC-ROC\n",
    "        cv=3,  # 3-fold cross-validation\n",
    "        verbose=2,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "\n",
    "    # Start the random search\n",
    "    print(\"\\nStarting GridSearchCV for Random Forest...\")\n",
    "    start = time()\n",
    "    try:\n",
    "        random_search.fit(X_train_tf, trainY)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during GridSearchCV: {e}\")\n",
    "        sys.exit(1)\n",
    "    end = time()\n",
    "    print(f\"GridSearchCV completed in {end - start:.2f}s\")\n",
    "\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    print(f\"\\nBest cross-validation AUC-ROC score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "    # Evaluate the model with the best parameters on the test set\n",
    "    best_rf_classifier = random_search.best_estimator_\n",
    "    print(\"\\nMaking predictions on the test set...\")\n",
    "    start = time()\n",
    "    try:\n",
    "        y_pred = best_rf_classifier.predict(X_test_tf)\n",
    "        if hasattr(best_rf_classifier, \"predict_proba\"):\n",
    "            y_pred_proba = best_rf_classifier.predict_proba(X_test_tf)[:, 1]  # Probability estimates for the positive class\n",
    "        else:\n",
    "            # If predict_proba is not available, use decision_function\n",
    "            y_pred_proba = best_rf_classifier.decision_function(X_test_tf)\n",
    "            y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min())  # Normalize to [0,1]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during prediction: {e}\")\n",
    "        sys.exit(1)\n",
    "    prediction_time = time() - start\n",
    "    print(f\"Prediction time: {prediction_time:.4f}s\")\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    acc = metrics.accuracy_score(testY, y_pred)\n",
    "    print(f\"\\nAccuracy on test set: {acc*100:.2f}%\")\n",
    "\n",
    "    # Calculate AUC-ROC\n",
    "    try:\n",
    "        auc = roc_auc_score(testY, y_pred_proba)\n",
    "        print(f\"AUC-ROC on test set: {auc:.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error computing AUC-ROC: {e}\")\n",
    "        auc = None\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification report for the optimized classifier: \\n\")\n",
    "    print(metrics.classification_report(testY, y_pred))\n",
    "\n",
    "    # Create a confusion matrix heatmap\n",
    "    conf_matrix = confusion_matrix(testY, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "\n",
    "    plt.title(\"Confusion Matrix Heatmap\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "\n",
    "    cm_path = os.path.join(output_dir, 'confusion_matrix_heatmap.png')\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix heatmap saved as '{cm_path}'\")\n",
    "\n",
    "    # Plot ROC Curve if AUC is computable\n",
    "    if auc is not None:\n",
    "        fpr, tpr, thresholds = roc_curve(testY, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        roc_path = os.path.join(output_dir, 'roc_curve.png')\n",
    "        plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"ROC curve saved as '{roc_path}'\")\n",
    "    else:\n",
    "        print(\"ROC curve was not plotted due to inability to compute AUC-ROC.\")\n",
    "\n",
    "    # Save the model and the vectorizer\n",
    "    try:\n",
    "        model_path = os.path.join(output_dir, 'best_rf_classifier_model.pkl')\n",
    "        with open(model_path, 'wb') as model_file:\n",
    "            pickle.dump(best_rf_classifier, model_file)\n",
    "        print(f\"Model saved as '{model_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the model: {e}\")\n",
    "\n",
    "    try:\n",
    "        vec_path = os.path.join(output_dir, 'tfidf_vectorizer.pkl')\n",
    "        with open(vec_path, 'wb') as vec_file:\n",
    "            pickle.dump(tf_vec, vec_file)\n",
    "        print(f\"TfidfVectorizer saved as '{vec_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the vectorizer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /home/dgxuser16/NTL/mccarthy/ahmad/Projects/ML_Course_Proj/data/twitter\n",
      "Output directory: output_new\n",
      "\n",
      "Training Dataset has 25000 rows and 2 columns\n",
      "Testing Dataset has 359 rows and 2 columns\n",
      "\n",
      "First 5 rows of Training Data:\n",
      "                                                  sentence  sentiment\n",
      "1110964  chillin at the rooftoop on a rainy sunday than...          1\n",
      "442422   me loves you too fran i don t feel too good ri...          0\n",
      "348915   u guys r so funny p boston tonight so close ma...          0\n",
      "575434   can t sleep i had that chance with that chunky...          0\n",
      "289960   i miss my tv it s at my apartment in san anton...          0\n",
      "\n",
      "Sentiment distribution in Training Data:\n",
      "sentiment\n",
      "0    12634\n",
      "1    12366\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 5 rows of Testing Data:\n",
      "                                            sentence  sentiment\n",
      "0  i loooooooovvvvvveee my kindle not that the dx...          1\n",
      "1  reading my kindle love it lee childs is good read          1\n",
      "2  ok first assesment of the kindle it fucking rocks          1\n",
      "3  you ll love your kindle i ve had mine for a fe...          1\n",
      "4  fair enough but i have the kindle and i think ...          1\n",
      "\n",
      "Sentiment distribution in Testing Data:\n",
      "sentiment\n",
      "1    182\n",
      "0    177\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Starting TF-IDF vectorization on training data...\n",
      "Time to transform training data: 0.25s\n",
      "Training Data Shape: n_samples=25000, n_features=23257\n",
      "\n",
      "Starting TF-IDF vectorization on testing data...\n",
      "Time taken to extract features from test data: 0.00 seconds\n",
      "Testing Data Shape: n_samples=359, n_features=23257\n",
      "\n",
      "Starting GridSearchCV for Random Forest...\n",
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=sqrt, n_estimators=100; total time= 1.5min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=sqrt, n_estimators=100; total time= 1.5min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=sqrt, n_estimators=100; total time= 1.5min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=sqrt, n_estimators=100; total time= 1.5min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=sqrt, n_estimators=100; total time= 1.5min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=sqrt, n_estimators=100; total time= 1.6min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=sqrt, n_estimators=100; total time= 1.6min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=sqrt, n_estimators=100; total time= 1.6min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=sqrt, n_estimators=100; total time= 1.6min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=log2, n_estimators=100; total time= 1.9min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=log2, n_estimators=100; total time= 2.0min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=log2, n_estimators=100; total time= 2.0min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=log2, n_estimators=100; total time= 2.0min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=log2, n_estimators=100; total time= 2.0min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=log2, n_estimators=100; total time= 2.0min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=log2, n_estimators=100; total time= 2.0min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=log2, n_estimators=100; total time= 2.0min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=log2, n_estimators=100; total time= 2.1min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=sqrt, n_estimators=200; total time= 3.1min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=sqrt, n_estimators=200; total time= 3.1min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=sqrt, n_estimators=200; total time= 3.1min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=sqrt, n_estimators=200; total time= 3.1min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=sqrt, n_estimators=200; total time= 3.1min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=sqrt, n_estimators=200; total time= 3.1min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=sqrt, n_estimators=200; total time= 3.2min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=sqrt, n_estimators=200; total time= 3.2min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=sqrt, n_estimators=200; total time= 3.2min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=log2, n_estimators=200; total time= 3.9min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=log2, n_estimators=200; total time= 3.9min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=log2, n_estimators=200; total time= 3.9min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=log2, n_estimators=200; total time= 3.9min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=log2, n_estimators=200; total time= 3.9min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=log2, n_estimators=200; total time= 4.0min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=log2, n_estimators=200; total time= 4.0min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=sqrt, n_estimators=100; total time= 2.4min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=sqrt, n_estimators=100; total time= 2.4min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=log2, n_estimators=200; total time= 4.0min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=sqrt, n_estimators=100; total time= 2.5min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=log2, n_estimators=200; total time= 4.1min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=sqrt, n_estimators=300; total time= 4.5min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=sqrt, n_estimators=300; total time= 4.6min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=sqrt, n_estimators=300; total time= 4.6min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=sqrt, n_estimators=300; total time= 4.7min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=sqrt, n_estimators=300; total time= 4.7min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=sqrt, n_estimators=300; total time= 4.8min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=sqrt, n_estimators=300; total time= 4.8min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=sqrt, n_estimators=300; total time= 4.8min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=sqrt, n_estimators=300; total time= 4.8min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=log2, n_estimators=100; total time= 3.0min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=log2, n_estimators=100; total time= 3.1min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=log2, n_estimators=100; total time= 3.1min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=log2, n_estimators=300; total time= 5.9min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=log2, n_estimators=300; total time= 5.9min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=log2, n_estimators=300; total time= 5.9min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=log2, n_estimators=300; total time= 5.9min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=log2, n_estimators=300; total time= 6.0min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=log2, n_estimators=300; total time= 6.0min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=log2, n_estimators=300; total time= 6.1min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=log2, n_estimators=300; total time= 6.1min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=log2, n_estimators=300; total time= 6.1min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=sqrt, n_estimators=100; total time= 2.4min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=sqrt, n_estimators=100; total time= 2.4min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=sqrt, n_estimators=100; total time= 2.4min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=sqrt, n_estimators=200; total time= 4.9min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=sqrt, n_estimators=200; total time= 5.0min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=sqrt, n_estimators=200; total time= 5.0min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=log2, n_estimators=100; total time= 3.0min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=log2, n_estimators=100; total time= 3.1min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=log2, n_estimators=100; total time= 3.0min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=log2, n_estimators=200; total time= 6.1min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=log2, n_estimators=200; total time= 6.1min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=log2, n_estimators=200; total time= 6.2min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=sqrt, n_estimators=100; total time= 2.4min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=sqrt, n_estimators=100; total time= 2.5min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=sqrt, n_estimators=100; total time= 2.5min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=sqrt, n_estimators=200; total time= 4.8min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=sqrt, n_estimators=200; total time= 4.8min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=sqrt, n_estimators=200; total time= 4.8min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=sqrt, n_estimators=300; total time= 7.4min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=sqrt, n_estimators=300; total time= 7.4min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=sqrt, n_estimators=300; total time= 7.3min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=log2, n_estimators=100; total time= 3.0min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=log2, n_estimators=100; total time= 3.0min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=log2, n_estimators=100; total time= 3.0min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=log2, n_estimators=200; total time= 6.0min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=log2, n_estimators=200; total time= 6.0min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=log2, n_estimators=200; total time= 6.1min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=sqrt, n_estimators=200; total time= 4.7min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=sqrt, n_estimators=200; total time= 4.8min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=sqrt, n_estimators=200; total time= 4.7min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=None, n_estimators=100; total time=11.0min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=sqrt, n_estimators=300; total time= 7.0min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=sqrt, n_estimators=300; total time= 7.0min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=None, n_estimators=100; total time=11.2min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=None, n_estimators=100; total time=11.2min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=log2, n_estimators=300; total time= 9.2min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=None, n_estimators=100; total time=11.2min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=sqrt, n_estimators=300; total time= 7.2min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=log2, n_estimators=300; total time= 9.3min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=None, n_estimators=100; total time=11.3min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=None, n_estimators=100; total time=11.4min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=None, n_estimators=100; total time=11.4min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=None, n_estimators=100; total time=11.4min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=None, n_estimators=100; total time=11.5min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=log2, n_estimators=300; total time= 8.9min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=sqrt, n_estimators=300; total time= 6.5min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=sqrt, n_estimators=300; total time= 6.7min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=log2, n_estimators=300; total time= 8.3min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=log2, n_estimators=200; total time= 5.9min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=log2, n_estimators=300; total time= 8.4min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=sqrt, n_estimators=300; total time= 6.8min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=log2, n_estimators=200; total time= 5.2min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=log2, n_estimators=200; total time= 5.9min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=log2, n_estimators=300; total time= 8.8min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=log2, n_estimators=300; total time= 7.5min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=log2, n_estimators=300; total time= 7.8min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=log2, n_estimators=300; total time= 7.9min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=None, n_estimators=100; total time=13.4min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=None, n_estimators=100; total time=14.1min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=None, n_estimators=100; total time=14.3min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=None, n_estimators=200; total time=18.4min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=None, n_estimators=100; total time=13.7min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=None, n_estimators=100; total time=13.5min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=None, n_estimators=200; total time=18.7min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=None, n_estimators=200; total time=18.8min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=None, n_estimators=200; total time=19.2min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=None, n_estimators=200; total time=19.4min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=None, n_estimators=200; total time=19.7min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=None, n_estimators=100; total time=14.9min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=None, n_estimators=200; total time=20.5min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=None, n_estimators=200; total time=20.5min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=None, n_estimators=200; total time=20.6min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=None, n_estimators=100; total time=12.6min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=None, n_estimators=100; total time=12.6min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=None, n_estimators=100; total time=12.9min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=None, n_estimators=300; total time=25.1min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=None, n_estimators=300; total time=26.2min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=None, n_estimators=300; total time=26.4min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=None, n_estimators=300; total time=26.5min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=None, n_estimators=300; total time=26.5min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=None, n_estimators=300; total time=25.5min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=None, n_estimators=200; total time=23.9min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=None, n_estimators=200; total time=24.1min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=None, n_estimators=200; total time=24.2min\n",
      "[CV] END bootstrap=True, criterion=gini, max_features=None, n_estimators=300; total time=27.6min\n",
      "[CV] END bootstrap=True, criterion=log_loss, max_features=None, n_estimators=300; total time=27.8min\n",
      "[CV] END bootstrap=True, criterion=entropy, max_features=None, n_estimators=300; total time=28.2min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=None, n_estimators=200; total time=23.4min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=None, n_estimators=200; total time=22.6min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=None, n_estimators=200; total time=24.1min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=None, n_estimators=200; total time=21.9min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=None, n_estimators=200; total time=22.6min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=None, n_estimators=200; total time=23.0min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=None, n_estimators=300; total time=32.0min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=None, n_estimators=300; total time=32.8min\n",
      "[CV] END bootstrap=False, criterion=gini, max_features=None, n_estimators=300; total time=33.7min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=None, n_estimators=300; total time=32.3min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=None, n_estimators=300; total time=32.4min\n",
      "[CV] END bootstrap=False, criterion=entropy, max_features=None, n_estimators=300; total time=33.8min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=None, n_estimators=300; total time=30.2min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=None, n_estimators=300; total time=31.0min\n",
      "[CV] END bootstrap=False, criterion=log_loss, max_features=None, n_estimators=300; total time=32.0min\n",
      "GridSearchCV completed in 2612.29s\n",
      "\n",
      "Best parameters found:\n",
      "{'bootstrap': True, 'criterion': 'entropy', 'max_features': 'log2', 'n_estimators': 300}\n",
      "\n",
      "Best cross-validation AUC-ROC score: 0.8322\n",
      "\n",
      "Making predictions on the test set...\n",
      "Prediction time: 0.8392s\n",
      "\n",
      "Accuracy on test set: 79.94%\n",
      "AUC-ROC on test set: 0.8762\n",
      "\n",
      "Classification report for the optimized classifier: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.76      0.79       177\n",
      "           1       0.78      0.84      0.81       182\n",
      "\n",
      "    accuracy                           0.80       359\n",
      "   macro avg       0.80      0.80      0.80       359\n",
      "weighted avg       0.80      0.80      0.80       359\n",
      "\n",
      "Confusion matrix heatmap saved as 'output_new/confusion_matrix_heatmap.png'\n",
      "ROC curve saved as 'output_new/roc_curve.png'\n",
      "Model saved as 'output_new/best_rf_classifier_model.pkl'\n",
      "TfidfVectorizer saved as 'output_new/tfidf_vectorizer.pkl'\n"
     ]
    }
   ],
   "source": [
    "main('/home/dgxuser16/NTL/mccarthy/ahmad/Projects/ML_Course_Proj/data/twitter', 'output_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from time import time\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def main(dataset_dir, output_dir):\n",
    "    # Importing the datasets\n",
    "    try:\n",
    "        df_train = pd.read_csv(os.path.join(dataset_dir, 'train_data.csv')).sample(n=100000, random_state=42)\n",
    "        df_test = pd.read_csv(os.path.join(dataset_dir, 'test_data.csv'))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f'Error: {e}')\n",
    "        sys.exit(1)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(f'Error: One of the CSV files is empty or malformed: {e}')\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f'An unexpected error occurred while reading the datasets: {e}')\n",
    "        sys.exit(1)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Dataset directory: {dataset_dir}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "    print(f\"\\nTraining data shape: {df_train.shape}\\n\")\n",
    "    print(f\"Testing data shape: {df_test.shape}\")\n",
    "\n",
    "    if 'sentiment' not in df_train.columns or 'sentiment' not in df_test.columns:\n",
    "        print(\"Error: 'sentiment' column not found in one of the datasets.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    trainX = df_train['sentence'].astype(str)\n",
    "    trainY = df_train['sentiment']\n",
    "    testX = df_test['sentence'].astype(str)\n",
    "    testY = df_test['sentiment']\n",
    "\n",
    "    tf_vec = TfidfVectorizer()\n",
    "    print(\"\\nStarting TF-IDF vectorization on training data...\")\n",
    "    start = time()\n",
    "    X_train_tf = tf_vec.fit_transform(trainX)\n",
    "    end = time()\n",
    "    print(f\"Time to transform training data: {end - start:.2f}s\")\n",
    "\n",
    "    print(\"\\nStarting TF-IDF vectorization on testing data...\")\n",
    "    start = time()\n",
    "    X_test_tf = tf_vec.transform(testX)\n",
    "    print(f\"Time taken to extract features from test data: {time() - start:.2f}s\")\n",
    "\n",
    "    # Initialize Random Forest Classifier with default parameters\n",
    "    rf_classifier = RandomForestClassifier(random_state=42, bootstrap=True, criterion='entropy', max_features='log2', n_estimators=300)\n",
    "    \n",
    "    print(\"\\nTraining Random Forest Classifier...\")\n",
    "    start = time()\n",
    "    try:\n",
    "        rf_classifier.fit(X_train_tf, trainY)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model training: {e}\")\n",
    "        sys.exit(1)\n",
    "    print(f\"Training completed in {time() - start:.2f}s\")\n",
    "\n",
    "    print(\"\\nMaking predictions on the test set...\")\n",
    "    start = time()\n",
    "    try:\n",
    "        y_pred = rf_classifier.predict(X_test_tf)\n",
    "        if hasattr(rf_classifier, \"predict_proba\"):\n",
    "            y_pred_proba = rf_classifier.predict_proba(X_test_tf)[:, 1]\n",
    "        else:\n",
    "            y_pred_proba = rf_classifier.decision_function(X_test_tf)\n",
    "            y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min())\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during prediction: {e}\")\n",
    "        sys.exit(1)\n",
    "    print(f\"Prediction time: {time() - start:.2f}s\")\n",
    "\n",
    "    acc = metrics.accuracy_score(testY, y_pred)\n",
    "    print(f\"\\nAccuracy on test set: {acc*100:.2f}%\")\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(testY, y_pred_proba)\n",
    "        print(f\"AUC-ROC on test set: {auc:.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error computing AUC-ROC: {e}\")\n",
    "        auc = None\n",
    "\n",
    "    print(\"\\nClassification report for the Random Forest classifier: \\n\")\n",
    "    print(metrics.classification_report(testY, y_pred))\n",
    "\n",
    "    conf_matrix = confusion_matrix(testY, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "\n",
    "    cm_path = os.path.join(output_dir, 'confusion_matrix_heatmap.png')\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix heatmap saved as '{cm_path}'\")\n",
    "\n",
    "    if auc is not None:\n",
    "        fpr, tpr, thresholds = roc_curve(testY, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        roc_path = os.path.join(output_dir, 'roc_curve.png')\n",
    "        plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"ROC curve saved as '{roc_path}'\")\n",
    "    else:\n",
    "        print(\"ROC curve was not plotted due to inability to compute AUC-ROC.\")\n",
    "\n",
    "    try:\n",
    "        model_path = os.path.join(output_dir, 'rf_classifier_model.pkl')\n",
    "        with open(model_path, 'wb') as model_file:\n",
    "            pickle.dump(rf_classifier, model_file)\n",
    "        print(f\"Model saved as '{model_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the model: {e}\")\n",
    "\n",
    "    try:\n",
    "        vec_path = os.path.join(output_dir, 'tfidf_vectorizer.pkl')\n",
    "        with open(vec_path, 'wb') as vec_file:\n",
    "            pickle.dump(tf_vec, vec_file)\n",
    "        print(f\"TfidfVectorizer saved as '{vec_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the vectorizer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /home/dgxuser16/NTL/mccarthy/ahmad/Projects/ML_Course_Proj/data/twitter\n",
      "Output directory: output_final\n",
      "\n",
      "Training data shape: (100000, 2)\n",
      "\n",
      "Testing data shape: (359, 2)\n",
      "\n",
      "Starting TF-IDF vectorization on training data...\n",
      "Time to transform training data: 1.09s\n",
      "\n",
      "Starting TF-IDF vectorization on testing data...\n",
      "Time taken to extract features from test data: 0.01s\n",
      "\n",
      "Training Random Forest Classifier...\n",
      "Training completed in 1609.89s\n",
      "\n",
      "Making predictions on the test set...\n",
      "Prediction time: 2.01s\n",
      "\n",
      "Accuracy on test set: 81.62%\n",
      "AUC-ROC on test set: 0.8820\n",
      "\n",
      "Classification report for the Random Forest classifier: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       177\n",
      "           1       0.81      0.84      0.82       182\n",
      "\n",
      "    accuracy                           0.82       359\n",
      "   macro avg       0.82      0.82      0.82       359\n",
      "weighted avg       0.82      0.82      0.82       359\n",
      "\n",
      "Confusion matrix heatmap saved as 'output_final/confusion_matrix_heatmap.png'\n",
      "ROC curve saved as 'output_final/roc_curve.png'\n",
      "Model saved as 'output_final/rf_classifier_model.pkl'\n",
      "TfidfVectorizer saved as 'output_final/tfidf_vectorizer.pkl'\n"
     ]
    }
   ],
   "source": [
    "main('/home/dgxuser16/NTL/mccarthy/ahmad/Projects/ML_Course_Proj/data/twitter', 'output_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
